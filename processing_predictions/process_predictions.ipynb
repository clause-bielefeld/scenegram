{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import json\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "def lemma2synset(lemma):\n",
    "    possible_synsets = [synset for synset in wn.synsets(lemma) if synset.pos() == \"n\"]\n",
    "    selected_synset = possible_synsets[0]\n",
    "    return selected_synset.name()\n",
    "\n",
    "\n",
    "def map_list_to_wn(lemma_list):\n",
    "    return list(map(lemma2synset, lemma_list))\n",
    "\n",
    "\n",
    "def process_file(file):\n",
    "\n",
    "    with open(file, \"r\") as f:\n",
    "        content = json.load(f)\n",
    "\n",
    "        args = content[\"args\"]\n",
    "        data = content[\"data\"]            \n",
    "\n",
    "    predictions_df = pd.DataFrame(data)\n",
    "\n",
    "    # add metadata\n",
    "    args[\"model_size\"] = re.search(r'\\-(\\d+b)\\-', args['model_id']).group(1)\n",
    "    predictions_df[\"model_id\"] = args[\"model_id\"]\n",
    "    predictions_df[\"model_size\"] = args[\"model_size\"]\n",
    "    predictions_df[\"quant\"] = args[\"quant\"]\n",
    "    predictions_df[\"model_type\"] = args[\"model_type\"]\n",
    "        \n",
    "    return args, predictions_df\n",
    "\n",
    "\n",
    "def process_files(files):\n",
    "    processed = [process_file(file) for file in files]\n",
    "\n",
    "    out_df = None\n",
    "\n",
    "    base_cols = [\n",
    "        \"item_id\",\n",
    "        \"tangram\",\n",
    "        \"scene\",\n",
    "        \"tangram_id\",\n",
    "        \"tangram_pos\",\n",
    "        \"image_name\",\n",
    "    ]\n",
    "    for args, df in processed:\n",
    "        if out_df is None:\n",
    "            out_df = df.copy()[base_cols]\n",
    "            \n",
    "        model_identifyer = f'{args[\"model_type\"]}-{args[\"model_size\"]}'\n",
    "        response_col = f'response_{model_identifyer}'\n",
    "        label_col = f'label_{model_identifyer}'\n",
    "        synset_col = f'synset_{model_identifyer}'\n",
    "        location_response_col = f'location_response_{model_identifyer}'\n",
    "        location_label_col = f'location_label_{model_identifyer}'\n",
    "        df = df.rename(columns={\"response\": response_col, \"label\": label_col, \"location_response\": location_response_col, \"location_label\": location_label_col})\n",
    "        df[synset_col] = df[label_col].map(map_list_to_wn)\n",
    "\n",
    "        cols = [\"item_id\", response_col, label_col, synset_col, location_response_col, location_label_col]\n",
    "        for col in cols: \n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "           \n",
    "        # handle missing values     \n",
    "        n_entries = df[response_col].map(len)\n",
    "        max_entries = max(n_entries)\n",
    "        too_few = n_entries < max_entries\n",
    "        too_few_selection = df[too_few]\n",
    "        \n",
    "        \n",
    "        if len(too_few_selection) > 0:\n",
    "            print(f'padding missing values in {model_identifyer} ({len(too_few_selection)} rows affected)')\n",
    "            print('item_ids:', too_few_selection.item_id.to_list())\n",
    "        \n",
    "        for i in too_few_selection.index:\n",
    "            missing = max_entries - n_entries.loc[i]\n",
    "            df.at[i, response_col] += ['None']*missing\n",
    "            df.at[i, label_col] += ['None']*missing\n",
    "            df.at[i, synset_col] += ['none.n.01']*missing\n",
    "\n",
    "        out_df = pd.merge(\n",
    "            out_df,\n",
    "            df[cols],\n",
    "            left_on=\"item_id\",\n",
    "            right_on=\"item_id\",\n",
    "        )\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = osp.abspath('../predicted_data/raw')\n",
    "output_dir = osp.abspath('../predicted_data/processed')\n",
    "\n",
    "if not osp.isdir(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twostep_predictions = True\n",
    "fewshot_predictions = False\n",
    "\n",
    "prediction_files = glob(osp.join(input_dir, '*.json'))\n",
    "\n",
    "if twostep_predictions:\n",
    "    prediction_files = [p for p in prediction_files if \"twostep\" in p]\n",
    "else: \n",
    "    prediction_files = [p for p in prediction_files if \"twostep\" not in p]\n",
    "    \n",
    "if fewshot_predictions:\n",
    "    prediction_files = [p for p in prediction_files if \"fewshot\" in p]\n",
    "else: \n",
    "    prediction_files = [p for p in prediction_files if \"fewshot\" not in p]\n",
    "    \n",
    "print(*prediction_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = process_files(prediction_files)\n",
    "\n",
    "predictions_df['set_idx'] = predictions_df.apply(lambda x: list(range(10)), axis=1)\n",
    "pred_cols = sorted([c for c in predictions_df.columns if any([\n",
    "    c.startswith(cat) for cat in ['response', 'label', 'synset']\n",
    "])])\n",
    "pred_cols.append('set_idx')\n",
    "\n",
    "predictions_df = predictions_df.explode(pred_cols)\n",
    "predictions_df['item_identifyer'] = predictions_df.apply(lambda x: f'{x.tangram_id}-{x.scene}-{x.set_idx}', axis=1)\n",
    "\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = osp.join(output_dir, f'processed_predictions{\"_twostep\" if twostep_predictions else \"\"}{\"_fewshot\" if fewshot_predictions else \"\"}.csv')\n",
    "\n",
    "print(f'write results to {out_path}')\n",
    "\n",
    "predictions_df.to_csv(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "context_representations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
