{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argilla as rg\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pickle\n",
    "\n",
    "slice = '0_0'\n",
    "\n",
    "data_dir = '../generated_data'\n",
    "out_dir = osp.abspath(f'{data_dir}/experiment_slices/results/{slice}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_item_identifier(row):\n",
    "    return f'{row.tangram_id}-{row.scene}-{row.workspace_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse credentials\n",
    "with open('group_argilla_credentials.sh', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    content_lines = [c.strip() for c in lines if \"=\" in c]\n",
    "    credentials = {\n",
    "        l.split('=')[0]: l.split('=')[1] \n",
    "        for l in content_lines\n",
    "    }\n",
    "    \n",
    "    \n",
    "# connect as owner to argilla server\n",
    "rg.init(\n",
    "    api_url=credentials['ARGILLA_API_URL'],\n",
    "    api_key=credentials['OWNER_API_KEY'],\n",
    "    #extra_headers={\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"}\n",
    ")\n",
    "\n",
    "# print owner info\n",
    "rg.User.me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "item_path = osp.join(data_dir, 'dense10_items.json')\n",
    "print(f'read items from {item_path} ...')\n",
    "item_df = pd.read_json(item_path).set_index('item_id')\n",
    "\n",
    "# get user info\n",
    "users = rg.User.list()\n",
    "users_dict = {u.id.hex : u.username for u in users}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get workspaces\n",
    "workspaces = rg.Workspace.list()\n",
    "\n",
    "scene_workspaces = [w for w in workspaces if w.name.startswith('sws')]\n",
    "baseline_workspaces = [w for w in workspaces if w.name.startswith('bws')]\n",
    "\n",
    "annotation_workspaces = scene_workspaces + baseline_workspaces\n",
    "\n",
    "print(f'{len(annotation_workspaces)} workspaces found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses = []  # init results list\n",
    "ann_errors = []\n",
    "\n",
    "for workspace in tqdm(annotation_workspaces, total=len(annotation_workspaces)):\n",
    "    # iterate through workspaces\n",
    "    workspace_name = workspace.name\n",
    "    dataset_name = f\"02_annotation_{workspace_name}\"\n",
    "\n",
    "    # load feedback dataset\n",
    "    feedback = rg.FeedbackDataset.from_argilla(dataset_name, workspace=workspace_name)\n",
    "    \n",
    "    for record in feedback.records:\n",
    "        # iterate through records in current dataset\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            record_metadata = record.metadata\n",
    "            item_id = record_metadata['item_id']\n",
    "            item_data = item_df.loc[item_id].to_dict()\n",
    "            \n",
    "            for response in record.responses:\n",
    "                # iterate through responses for current record\n",
    "                \n",
    "                user_id = response.user_id.hex\n",
    "                user_name = users_dict[user_id]\n",
    "                raw_annotation = response.values['response'].value\n",
    "                response_status = response.status\n",
    "                response_time = response.updated_at\n",
    "\n",
    "                # merge the data into one dict\n",
    "                response_data = {\n",
    "                    **item_data,\n",
    "                    **record_metadata,\n",
    "                    'user_id': user_id,\n",
    "                    'user_name': user_name,\n",
    "                    'raw_annotation': raw_annotation,\n",
    "                    'status': response_status,\n",
    "                    'time': response_time\n",
    "                }\n",
    "\n",
    "                # append dict to results list\n",
    "                all_responses.append(response_data)\n",
    "                \n",
    "        except:\n",
    "            ann_errors.append((feedback.name, dict(record)))\n",
    "            \n",
    "annotation_df = pd.DataFrame(all_responses)\n",
    "annotation_df['item_identifyer'] = annotation_df.apply(lambda x: make_item_identifier(x), axis=1)\n",
    "annotation_df = annotation_df.sort_values(by=['item_id', 'workspace_name']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_out_path = osp.join(out_dir, 'collected_annotations.csv')\n",
    "pkl_out_path = osp.join(out_dir, 'collected_annotations.pkl')\n",
    "\n",
    "# merge with existing annotations\n",
    "\n",
    "if osp.isfile(pkl_out_path):\n",
    "    with open(pkl_out_path, 'rb') as f:\n",
    "        prev_annotation_df = pickle.load(f)\n",
    "        \n",
    "    if 'item_identifyer' not in prev_annotation_df.columns:\n",
    "        prev_annotation_df['item_identifyer'] = prev_annotation_df.apply(lambda x: make_item_identifier(x), axis=1)\n",
    "    previous_records_df = prev_annotation_df[~prev_annotation_df.item_identifyer.isin(annotation_df.item_identifyer)]\n",
    "    merged_annotation_df = pd.concat([previous_records_df, annotation_df]).reset_index(drop=True)\n",
    "else:\n",
    "    merged_annotation_df = annotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv\n",
    "merged_annotation_df.to_csv(csv_out_path)\n",
    "\n",
    "# save pkl\n",
    "with open(pkl_out_path, 'wb') as f:\n",
    "    pickle.dump(merged_annotation_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_datasets = [d for d in rg.list_datasets() if not d.name.startswith('02_')]\n",
    "info_datasets = [d for d in other_datasets if d.name.startswith('01_info')]\n",
    "completion_datasets = [d for d in other_datasets if d.name.startswith('03_completion')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prolific_ids = []\n",
    "id_errors = []\n",
    "\n",
    "for dataset in info_datasets:\n",
    "    feedback = rg.FeedbackDataset.from_argilla(dataset.name, workspace=dataset.workspace.name)\n",
    "    for record in feedback.records:\n",
    "        \n",
    "        try:\n",
    "            record_metadata = record.metadata\n",
    "            for response in record.responses:\n",
    "                user_id = response.user_id.hex\n",
    "                user_name = users_dict[user_id]\n",
    "                prolific_id = response.values['prolific_id'].value\n",
    "                response_status = response.status\n",
    "                response_time = response.updated_at\n",
    "                \n",
    "                response_data = {\n",
    "                    **record_metadata,\n",
    "                    'user_id': user_id,\n",
    "                    'user_name': user_name,\n",
    "                    'prolific_id': prolific_id,\n",
    "                    'status': response_status,\n",
    "                    'time': response_time\n",
    "                }\n",
    "                \n",
    "                user_prolific_ids.append(response_data)\n",
    "        except:\n",
    "            id_errors.append((feedback.name, dict(record)))\n",
    "            \n",
    "prolific_ids_df = pd.DataFrame(user_prolific_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prolific_ids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_out_path = osp.join(out_dir, 'prolific_ids.csv')\n",
    "\n",
    "# merge with existing data\n",
    "if osp.isfile(csv_out_path):\n",
    "    prev_prolific_ids_df = pd.read_csv(csv_out_path, index_col=0)\n",
    "    prev_ids_df = prev_prolific_ids_df[~prev_prolific_ids_df.user_id.isin(prolific_ids_df.user_id)]\n",
    "    merged_ids_df = pd.concat([prev_ids_df, prolific_ids_df]).reset_index(drop=True)\n",
    "else:\n",
    "    merged_ids_df = prolific_ids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prolific_ids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv\n",
    "merged_ids_df.to_csv(csv_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_data = []\n",
    "completion_errors = []\n",
    "\n",
    "for dataset in tqdm(completion_datasets):\n",
    "    feedback = rg.FeedbackDataset.from_argilla(dataset.name, workspace=dataset.workspace.name)\n",
    "    for record in feedback.records:\n",
    "        record_metadata = record.metadata\n",
    "        for response in record.responses:\n",
    "            try:\n",
    "                user_id = response.user_id.hex\n",
    "                user_name = users_dict[user_id]\n",
    "                submission_confirmation = response.values['submission_confirmation'].value\n",
    "                comments = response.values.get('comments', None)\n",
    "                if comments is not None:\n",
    "                    comments = comments.value\n",
    "                response_status = response.status\n",
    "                response_time = response.updated_at\n",
    "                \n",
    "                response_data = {\n",
    "                    **record_metadata,\n",
    "                    'user_id': user_id,\n",
    "                    'user_name': user_name,\n",
    "                    'submission_confirmation': submission_confirmation,\n",
    "                    'comments': comments,\n",
    "                    'status': response_status,\n",
    "                    'time': response_time\n",
    "                }\n",
    "                \n",
    "                completion_data.append(response_data)\n",
    "            except: \n",
    "                completion_errors.append((feedback.name, dict(record)))\n",
    "            \n",
    "completion_df = pd.DataFrame(completion_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_out_path = osp.join(out_dir, 'completion_data.csv')\n",
    "\n",
    "# merge with existing data\n",
    "if osp.isfile(csv_out_path):\n",
    "    prev_completion_df = pd.read_csv(csv_out_path, index_col=0)\n",
    "    prev_comp_df = prev_completion_df[~prev_completion_df.user_id.isin(completion_df.user_id)]\n",
    "    merged_completion_df = pd.concat([prev_comp_df, completion_df]).reset_index(drop=True)\n",
    "else: \n",
    "    merged_completion_df = completion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_completion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv\n",
    "merged_completion_df.to_csv(csv_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
