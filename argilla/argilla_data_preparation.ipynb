{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla_utils import make_password, make_name\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "from random import sample, seed, shuffle\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "\n",
    "# Argilla & data settings\n",
    "RESET_ARGILLA = True        # delete workspaces, users and datasets\n",
    "TEST_MODE = False           # restrict dataset size\n",
    "if TEST_MODE:\n",
    "    TEST_K_SCENES = 5\n",
    "    TEST_K_TANGRAMS = 14\n",
    "MODE = 'grid'               # 'inline', 'side' or 'grid', default: 'grid'\n",
    "assert MODE in {'inline', 'side', 'grid'}, f'invalid mode: {MODE}'\n",
    "N_GROUPS = 2                # number of groups between which the tangrams are distributed, default: 2\n",
    "ANNOTATIONS_PER_ITEM = 10   #  default: 10\n",
    "\n",
    "# random seed\n",
    "RANDOM_STATE = 123\n",
    "seed(RANDOM_STATE)\n",
    "\n",
    "# paths\n",
    "data_dir = osp.abspath('../generated_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_slices(slices, reference_df):\n",
    "    # ensure tangram ids are unique within slices\n",
    "    for i, slice in enumerate(slices):\n",
    "        if not slice.groupby('tangram_id').size().max() == 1:\n",
    "            raise Exception(f'tangram ids not unique in slice {i}')\n",
    "    # ensure all data is contained in slices\n",
    "    try:\n",
    "        pd.testing.assert_frame_equal(\n",
    "            pd.concat(slices).sort_index(), \n",
    "            reference_df.sort_index()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise Exception(f'scene partitions differ from reference df with error message: {e}')\n",
    "        \n",
    "    return True\n",
    "\n",
    "\n",
    "def tangram_group_idx(tangram_id, partition_idx):\n",
    "    mask =  np.array([tangram_id in pi for pi in partition_idx]).astype(int)\n",
    "    assert mask.sum() == 1\n",
    "    return mask.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "item_path = osp.join(data_dir, 'dense10_items.json')\n",
    "print(f'read items from {item_path} ...')\n",
    "item_df = pd.read_json(item_path)\n",
    "\n",
    "scene_items = item_df.loc[item_df.scene != 'none']\n",
    "baseline_items = item_df.loc[item_df.scene == 'none']\n",
    "\n",
    "print(f'total items: {len(item_df)}')\n",
    "print(f'scene items: {len(scene_items)}')\n",
    "print(f'baseline items: {len(baseline_items)}')\n",
    "assert len(scene_items) + len(baseline_items) == len(item_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    \n",
    "    n_scenes = len(pd.unique(scene_items.scene))\n",
    "    n_tangram_ids = len(pd.unique(scene_items.tangram_id))\n",
    "    TEST_K_SCENES = TEST_K_SCENES if TEST_K_SCENES > 0 else n_scenes\n",
    "    TEST_K_TANGRAMS = TEST_K_TANGRAMS if TEST_K_TANGRAMS > 0 else n_tangram_ids\n",
    "    \n",
    "    print(f'restrict to {TEST_K_SCENES} scenes and {TEST_K_TANGRAMS} tangrams for testing')\n",
    "    \n",
    "    unique_scenes = pd.unique(scene_items.scene).tolist()\n",
    "    unique_tangrams = pd.unique(scene_items.tangram_id).tolist()\n",
    "    scene_selection = sample(unique_scenes, k=TEST_K_SCENES)\n",
    "    tangram_selection = sample(unique_tangrams, k=TEST_K_TANGRAMS)\n",
    "\n",
    "    scene_items = scene_items.loc[scene_items.scene.isin(scene_selection)]\n",
    "    scene_items = scene_items.loc[scene_items.tangram_id.isin(tangram_selection)]\n",
    "    baseline_items = baseline_items.loc[baseline_items.tangram_id.isin(tangram_selection)]\n",
    "\n",
    "    print(f'number of selected items: {len(scene_items)} scene items, {len(baseline_items)} baseline items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Workspaces and Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make workspaces\n",
    "\n",
    "ann_workspaces = []\n",
    "\n",
    "n_baseline_workspaces = N_GROUPS * ANNOTATIONS_PER_ITEM\n",
    "print(f'create {n_baseline_workspaces} baseline workspaces (for {N_GROUPS} groups, {ANNOTATIONS_PER_ITEM} anns per item)...')\n",
    "partition_idx = list(range(N_GROUPS))\n",
    "annotator_idx = list(range(ANNOTATIONS_PER_ITEM))\n",
    "for p_idx, a_idx in list(product(partition_idx, annotator_idx)):\n",
    "    ann_workspaces.append(f'bws{p_idx}_{a_idx}')\n",
    "    \n",
    "n_scenes = len(pd.unique(scene_items.scene))\n",
    "n_scene_workspaces = n_scenes * N_GROUPS * ANNOTATIONS_PER_ITEM\n",
    "print(f'create {n_scene_workspaces} scene workspaces (for {n_scenes} scenes and {N_GROUPS} groups, {ANNOTATIONS_PER_ITEM} anns per item)...')\n",
    "partition_idx = list(range(n_scenes * N_GROUPS))\n",
    "annotator_idx = list(range(ANNOTATIONS_PER_ITEM))\n",
    "for p_idx, a_idx in list(product(partition_idx, annotator_idx)):\n",
    "    ann_workspaces.append(f'sws{p_idx}_{a_idx}')\n",
    "    \n",
    "print(f'total number of annotation workspaces: {len(ann_workspaces)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make annotator credentials\n",
    "\n",
    "all_users = []\n",
    "\n",
    "print(f'create users and credentials for {len(ann_workspaces)} workspaces ({ANNOTATIONS_PER_ITEM} annotations per item)...')\n",
    "\n",
    "# make credentials\n",
    "for workspace in ann_workspaces:\n",
    "        \n",
    "    user_name = ''\n",
    "    while user_name == '' or user_name in [u['username'] for u in all_users]:\n",
    "        # ensure that generated user names are valid\n",
    "        user_name = make_name()\n",
    "    password = make_password()\n",
    "    \n",
    "    partition_name, partition_annotator_idx = workspace.split('_')\n",
    "    \n",
    "    all_users.append({\n",
    "        'username': user_name,\n",
    "        'password': password,\n",
    "        'workspace': workspace,\n",
    "        'partition': partition_name,\n",
    "        'partition_annotator_idx': int(partition_annotator_idx), \n",
    "        'valid': True\n",
    "    })\n",
    "    \n",
    "user_df = pd.DataFrame(all_users)\n",
    "user_df = user_df.reset_index()\n",
    "\n",
    "user_filepath = osp.join(data_dir, 'argilla_users.csv')\n",
    "\n",
    "print(f'save credentials to {user_filepath} ...')\n",
    "user_df.to_csv(user_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tangrams = pd.unique(scene_items.tangram_id)\n",
    "unique_scenes = pd.unique(scene_items.scene)\n",
    "\n",
    "n_scenes = len(unique_tangrams)\n",
    "n_tangrams = len(unique_scenes)\n",
    "\n",
    "shuffled_tangrams = unique_tangrams.tolist()\n",
    "shuffle(shuffled_tangrams)\n",
    "\n",
    "group_tangrams = np.array_split(shuffled_tangrams, N_GROUPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'creating baseline splits for {n_tangrams} tangrams and {N_GROUPS} groups...')\n",
    "\n",
    "baseline_partitions = []\n",
    "for tangram_ids in group_tangrams:\n",
    "    baseline_partition = baseline_items.loc[baseline_items.tangram_id.isin(tangram_ids)]\n",
    "    baseline_partitions.append(baseline_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'creating splits for {n_tangrams} tangrams, {n_scenes} scenes and {N_GROUPS} groups...')\n",
    "\n",
    "# shuffle items and sort by tangram_id -> tangram_ids clustered together, but with permuted scenes\n",
    "shuffled_scene_items  = scene_items.sample(frac=1, random_state=RANDOM_STATE).sort_values(by='tangram_id')\n",
    "\n",
    "# create slices containing each tangram once with a random scene\n",
    "tangram_slices = [\n",
    "    shuffled_scene_items[i::n_tangrams]  \n",
    "    # step size is n_tangrams, \n",
    "    # i.e. select entry per tangram (starting with offset i)\n",
    "    for i in range(n_tangrams)\n",
    "]\n",
    "\n",
    "# split up the tangram slices for annotation groups\n",
    "scene_partitions = []\n",
    "for tangram_slice in tangram_slices:\n",
    "    partitioned_slices = [tangram_slice.loc[tangram_slice.tangram_id.isin(g_idx)] for g_idx in group_tangrams]\n",
    "    scene_partitions += partitioned_slices\n",
    "\n",
    "# ensure everything is OK\n",
    "assert validate_slices(scene_partitions, scene_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_partition_dict = {f'bws{i}': p for i, p in enumerate(baseline_partitions)}\n",
    "scene_partition_dict = {f'sws{i}': p for i, p in enumerate(scene_partitions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_workspaces = [w for w in ann_workspaces if 'bws' in w]\n",
    "scene_workspaces = [w for w in ann_workspaces if 'sws' in w]\n",
    "\n",
    "assert len(baseline_workspaces) == len(baseline_partitions) * ANNOTATIONS_PER_ITEM\n",
    "assert len(scene_workspaces) == len(scene_partitions) * ANNOTATIONS_PER_ITEM\n",
    "\n",
    "all_workspaces = baseline_workspaces + scene_workspaces\n",
    "all_partitions = baseline_partitions + scene_partitions\n",
    "\n",
    "workspace_partition_map = {w: w.split('_')[0] for w in all_workspaces}\n",
    "\n",
    "print(f'# baseline workspaces/partitions: {len(baseline_workspaces)} / {len(baseline_partitions)}')\n",
    "print(f'# scene workspaces/partitions: {len(scene_workspaces)} / {len(scene_partitions)}')\n",
    "print(f'# all workspaces/partitions: {len(all_workspaces)} / {len(baseline_partitions + scene_partitions)}')\n",
    "\n",
    "output_partitions = []\n",
    "\n",
    "for k, v in {**baseline_partition_dict, **scene_partition_dict}.items():\n",
    "    part = v.copy()\n",
    "    part['partition'] = k\n",
    "    part['workspaces'] = [[w for w, p in workspace_partition_map.items() if p == k]] * len(part.index)\n",
    "    \n",
    "    output_partitions.append(part)\n",
    "    \n",
    "partition_df = pd.concat(output_partitions).reset_index(drop=True)\n",
    "partition_df['tangram_group'] = partition_df.tangram_id.map(lambda x: tangram_group_idx(x, group_tangrams))\n",
    "\n",
    "partition_path = osp.join(data_dir, 'argilla_partitions.csv')\n",
    "partition_df.to_csv(partition_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_stats = pd.DataFrame(index=pd.unique(partition_df.partition))\n",
    "\n",
    "partition_group_map = partition_df.groupby('partition').agg({'tangram_group': set})\n",
    "assert not False in partition_group_map.map(lambda x: len(x) == 1)\n",
    "partition_group_map = partition_group_map.tangram_group.explode()\n",
    "\n",
    "min_vals = partition_df.groupby('partition').agg({'kilogram_snd': np.min}).kilogram_snd\n",
    "max_vals = partition_df.groupby('partition').agg({'kilogram_snd': np.max}).kilogram_snd\n",
    "mean_vals = partition_df.groupby('partition').agg({'kilogram_snd': np.mean}).kilogram_snd\n",
    "std_vals = partition_df.groupby('partition').agg({'kilogram_snd': np.std}).kilogram_snd\n",
    "scene_counts = partition_df.groupby('partition').agg({'scene': lambda x: dict(Counter(x))}).scene\n",
    "tangram_ids = partition_df.groupby('partition').agg({'tangram_id': set}).tangram_id\n",
    "\n",
    "partition_stats['partition_group'] = partition_group_map\n",
    "partition_stats['snd_min'] = min_vals\n",
    "partition_stats['snd_max'] = max_vals\n",
    "partition_stats['snd_mean'] = mean_vals\n",
    "partition_stats['snd_std'] = std_vals\n",
    "partition_stats['tangram_ids'] = tangram_ids\n",
    "partition_stats['scenes'] = scene_counts\n",
    "\n",
    "stats_path = osp.join(data_dir, 'argilla_partition_stats.csv')\n",
    "partition_stats.to_csv(stats_path)\n",
    "\n",
    "partition_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
