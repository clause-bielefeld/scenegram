{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "from argilla_utils import build_info_dataset, build_completion_dataset, build_annotation_dataset, make_password, make_name\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "import os.path as osp\n",
    "import datetime\n",
    "\n",
    "MODE = 'grid'               # 'inline', 'side' or 'grid', default: 'grid'\n",
    "\n",
    "# Prolific settings\n",
    "COMPLETION_CODE = \"CODE\"\n",
    "COMPLETION_URL = \"https://app.prolific.com/submissions/complete?cc=CODE\"\n",
    "\n",
    "# random seed\n",
    "RANDOM_STATE = 123\n",
    "\n",
    "# paths\n",
    "data_dir = osp.abspath('../generated_data/')\n",
    "experiment_slice_dir = osp.join(data_dir, 'experiment_slices')\n",
    "backup_dir = osp.join(experiment_slice_dir, 'backups')\n",
    "credential_file = 'group_argilla_credentials.sh'\n",
    "\n",
    "full_partition_file = 'argilla_partitions.csv'\n",
    "full_user_file = 'argilla_users.csv'\n",
    "\n",
    "slice_partition_file = 'argilla_partitions_0.csv'\n",
    "slice_user_file = 'argilla_users_0_0.csv'\n",
    "other_slice_user_file = f'argilla_users_0{\"_1\" if \"_0.\" in slice_user_file else \"_0\"}.csv'\n",
    "\n",
    "add_partition_file = f'ADD_{slice_partition_file}'\n",
    "add_user_file = f'ADD_{slice_user_file}'\n",
    "\n",
    "copy_from_user = 'gxxwaytb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse credentials\n",
    "with open(credential_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    content_lines = [c.strip() for c in lines if \"=\" in c]\n",
    "    credentials = {\n",
    "        l.split('=')[0]: l.split('=')[1] \n",
    "        for l in content_lines\n",
    "    }\n",
    "    \n",
    "    \n",
    "# connect as owner to argilla server\n",
    "rg.init(\n",
    "    api_url=credentials['ARGILLA_API_URL'],\n",
    "    api_key=credentials['OWNER_API_KEY'],\n",
    "    #extra_headers={\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"}\n",
    ")\n",
    "\n",
    "# print owner info\n",
    "print(rg.User.me())\n",
    "\n",
    "print(credentials['ARGILLA_API_URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_user_filepath = osp.join(experiment_slice_dir, slice_user_file)\n",
    "print(f'load slice credentials from {slice_user_filepath} ...')\n",
    "slice_user_df = pd.read_csv(slice_user_filepath, index_col=0)\n",
    "\n",
    "other_slice_user_filepath = osp.join(experiment_slice_dir, other_slice_user_file)\n",
    "print(f'load other slice credentials from {other_slice_user_filepath} ...')\n",
    "other_slice_user_df = pd.read_csv(other_slice_user_filepath, index_col=0)\n",
    "\n",
    "slice_partition_filepath = osp.join(experiment_slice_dir, slice_partition_file)\n",
    "print(f'load slice partitions from {slice_partition_filepath} ...')\n",
    "slice_partition_df = pd.read_csv(slice_partition_filepath, index_col=0)\n",
    "\n",
    "full_user_filepath = osp.join(data_dir, full_user_file)\n",
    "print(f'load slice credentials from {full_user_filepath} ...')\n",
    "full_user_df = pd.read_csv(full_user_filepath, index_col=0)\n",
    "\n",
    "full_partition_filepath = osp.join(data_dir, full_partition_file)\n",
    "print(f'load slice partitions from {full_partition_filepath} ...')\n",
    "full_partition_df = pd.read_csv(full_partition_filepath, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get entry for origin user\n",
    "\n",
    "user_entry = slice_user_df.loc[slice_user_df.username == copy_from_user]\n",
    "assert len(user_entry) == 1\n",
    "user_entry = user_entry.iloc[0]\n",
    "user_idx = user_entry.name\n",
    "\n",
    "user_workspace = user_entry.workspace\n",
    "user_partition = user_entry.partition\n",
    "\n",
    "# select new workspace\n",
    "\n",
    "full_and_slice_df = pd.concat([full_user_df, slice_user_df, other_slice_user_df])\n",
    "\n",
    "partition_users = full_and_slice_df.loc[full_and_slice_df.partition == user_partition]\n",
    "partition_user_workspaces = partition_users.workspace.tolist()\n",
    "all_partition_annotator_idx = list(map(lambda x: int(x.split('_')[1]), partition_user_workspaces))\n",
    "\n",
    "new_user_partition_annotator_idx = max(all_partition_annotator_idx) + 1\n",
    "new_user_workspace = f'{user_partition}_{new_user_partition_annotator_idx}'\n",
    "\n",
    "workspace_partition_map = {new_user_workspace: user_partition}\n",
    "\n",
    "# make new username\n",
    "\n",
    "new_user_name = ''\n",
    "existing_names = full_and_slice_df.username.tolist()\n",
    "while new_user_name == '' or new_user_name in existing_names:\n",
    "    # ensure that generated user names are valid\n",
    "    new_user_name = make_name()\n",
    "new_password = make_password()\n",
    "\n",
    "assert new_user_name not in existing_names\n",
    "\n",
    "# make new entry\n",
    "\n",
    "new_user_entry = user_entry.copy()\n",
    "new_user_entry.username = new_user_name\n",
    "new_user_entry.password = new_password\n",
    "new_user_entry.workspace = new_user_workspace\n",
    "new_user_entry.partition_annotator_idx = new_user_partition_annotator_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save backup of user file\n",
    "\n",
    "time = datetime.datetime.now()\n",
    "time_str = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "backup_filename = slice_user_file.replace('.csv', f'_SAVE_{time_str}.csv')\n",
    "\n",
    "if not osp.isdir(backup_dir):\n",
    "    os.makedirs(backup_dir)\n",
    "\n",
    "backup_path = osp.join(backup_dir, backup_filename)\n",
    "print(f'saving backup to {backup_path}')\n",
    "slice_user_df.to_csv(backup_path)\n",
    "\n",
    "# set valid flag to false\n",
    "slice_user_df.loc[user_idx, 'valid'] = False\n",
    "\n",
    "# update slice df\n",
    "new_slice_user_df = pd.concat([\n",
    "    slice_user_df,\n",
    "    pd.DataFrame(new_user_entry).T\n",
    "])\n",
    "\n",
    "new_slice_user_df.to_csv(slice_user_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make workspace\n",
    "rg.Workspace.create(new_user_workspace)\n",
    "\n",
    "# make user\n",
    "rg.User.create(\n",
    "        username=new_user_entry.username,\n",
    "        password=new_user_entry.password,\n",
    "        workspaces=[new_user_entry.workspace],\n",
    "        role=\"annotator\",\n",
    "    )\n",
    "\n",
    "# select workspace obj (for dataset creation)\n",
    "workspaces = [w for w in rg.Workspace.list() if w.name == new_user_workspace]\n",
    "assert len(workspaces) == 1\n",
    "workspace = workspaces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build datasets\n",
    "info_dataset = build_info_dataset(workspace)\n",
    "completion_dataset = build_completion_dataset(workspace, COMPLETION_CODE, COMPLETION_URL)\n",
    "annotation_dataset = build_annotation_dataset(workspace, workspace_partition_map, slice_partition_df, credentials['IMG_LOCATION'], MODE, RANDOM_STATE)\n",
    "\n",
    "# push info, completion and annotation datasets to workspace (in reversed order)\n",
    "completion_dataset.push_to_argilla(name=completion_dataset[0].metadata['dataset_name'], workspace=workspace.name, show_progress=False)\n",
    "annotation_dataset.push_to_argilla(name=annotation_dataset[0].metadata['dataset_name'], workspace=workspace.name, show_progress=False)\n",
    "info_dataset.push_to_argilla(name=info_dataset[0].metadata['dataset_name'], workspace=workspace.name, show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
