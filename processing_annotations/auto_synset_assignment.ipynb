{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = osp.abspath('../collected_data/processed/')\n",
    "\n",
    "full_path = osp.join(data_dir, 'full_processed_collected_data.csv')\n",
    "full_df = pd.read_csv(full_path, index_col=0)\n",
    "\n",
    "print(f'{len(full_df)} entries in full df')\n",
    "\n",
    "valid_path = osp.join(data_dir, 'valid_processed_collected_data.csv')\n",
    "valid_df = pd.read_csv(valid_path, index_col=0)\n",
    "\n",
    "print(f'{len(valid_df)} entries in valid df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_with_comments_df = valid_df[~valid_df.comments.isna()]\n",
    "print(f'{len(valid_with_comments_df)} entries with comments in valid df')\n",
    "\n",
    "columns = ['item_identifyer', 'raw_annotation', 'clean_annotation', 'head_noun', 'comments']\n",
    "comments_path = osp.join(data_dir, 'valid_commented_collected_data.csv')\n",
    "valid_with_comments_df[columns].to_csv(comments_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.groupby('tangram_id').size().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.groupby('scene').size().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.groupby('item_id').size().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "\n",
    "def get_first_head_noun(name):\n",
    "    possible_names = name.split('/')\n",
    "    selected_name = possible_names[0].strip().lower()\n",
    "    return selected_name\n",
    "\n",
    "def get_possible_synsets(name):\n",
    "    return [s for s in wn.synsets(name) if s.pos() == 'n']\n",
    "\n",
    "synset_df = valid_df[['item_identifyer', 'tangram', 'scene', 'raw_annotation', 'clean_annotation', 'head_noun', 'comments', 'image_url']]\n",
    "\n",
    "synset_df['head_noun'] = synset_df['head_noun'].map(get_first_head_noun)\n",
    "\n",
    "# get synsets\n",
    "synset_df['possible_synsets'] = synset_df['head_noun'].map(get_possible_synsets)\n",
    "synset_df['selected_synset'] = synset_df['possible_synsets'].map(lambda x: x[0] if len(x) > 0 else '')\n",
    "synset_df['synset_definition'] = synset_df['selected_synset'].map(lambda x: x.definition() if type(x) == Synset else '')\n",
    "\n",
    "# convert synsets to strings\n",
    "synset_df['possible_synsets'] = synset_df['possible_synsets'].map(lambda l_s: [s.name() for s in l_s])\n",
    "synset_df['selected_synset'] = synset_df['selected_synset'].map(lambda x: x.name() if type(x) == Synset else x)\n",
    "synset_df = synset_df.sort_values(by=['tangram', 'scene', 'head_noun']).reset_index()\n",
    "\n",
    "# reorder columns\n",
    "synset_df['corrected_head_noun'] = ''\n",
    "synset_df = synset_df[[\n",
    "    'item_identifyer', 'raw_annotation', 'clean_annotation', 'head_noun', 'corrected_head_noun', 'selected_synset', 'synset_definition', 'possible_synsets', 'comments', 'image_url'\n",
    "]]\n",
    "\n",
    "\n",
    "synset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df in two equal sized parts\n",
    "half = len(synset_df) // 2\n",
    "synset_df_0 = synset_df.iloc[:half]\n",
    "synset_df_1 = synset_df.iloc[half:]\n",
    "\n",
    "# check if combined splits are equal to original df\n",
    "assert synset_df.equals(pd.concat([synset_df_0, synset_df_1]))\n",
    "\n",
    "for i, df in enumerate([synset_df_0, synset_df_1]):\n",
    "    file_name = f'valid_processed_synsets_{i}_auto.csv'\n",
    "    file_path = osp.join(data_dir, file_name)\n",
    "\n",
    "    print(f'write df to {file_path}')\n",
    "    df.to_csv(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "context_representations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
