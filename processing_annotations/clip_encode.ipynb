{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: success\n",
      "processor: success\n"
     ]
    }
   ],
   "source": [
    "clip_path = osp.abspath('../analyzing_annotations/clip_model')\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=clip_path)\n",
    "print('model: success')\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=clip_path)\n",
    "print('processor: success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_features(s, clip_model=model, clip_processor=processor, return_numpy=True):\n",
    "    inputs = clip_processor(text=s, return_tensors='pt')\n",
    "    outputs = clip_model.get_text_features(**inputs).squeeze()\n",
    "    return outputs.detach().numpy() if return_numpy else outputs\n",
    "\n",
    "\n",
    "def get_image_features(img, clip_model=model, clip_processor=processor, return_numpy=True):\n",
    "    inputs = clip_processor(images=img, return_tensors='pt')\n",
    "    outputs = clip_model.get_image_features(**inputs).squeeze()\n",
    "    return outputs.detach().numpy() if return_numpy else outputs\n",
    "\n",
    "\n",
    "def get_features_for_img_path(img_path, clip_model=model, clip_processor=processor, return_numpy=True):\n",
    "    img = Image.open(img_path)\n",
    "    return get_image_features(img, clip_model, clip_processor, return_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = osp.abspath('../collected_data/')\n",
    "processed_dir = osp.join(input_dir, 'processed')\n",
    "kilogram_dir = osp.abspath('../kilogram')\n",
    "\n",
    "output_dir = osp.join(processed_dir, 'clip_encodings')\n",
    "if not osp.isdir(output_dir):\n",
    "    print(f'make directory {output_dir}')\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(s): \n",
    "    return s.strip().lower()\n",
    "\n",
    "def clean_raw_ann(s):\n",
    "    s = re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
    "    return s.strip().lower()\n",
    "\n",
    "input_file = osp.join(processed_dir, 'final_processed_data.csv')\n",
    "\n",
    "ann_df = pd.read_csv(input_file, index_col=0)\n",
    "\n",
    "ann_df.raw_annotation = ann_df.raw_annotation.map(clean_raw_ann)\n",
    "ann_df.clean_annotation = ann_df.clean_annotation.map(clean_str)\n",
    "ann_df.head_noun = ann_df.head_noun.map(clean_str)\n",
    "\n",
    "tangrams = sorted(pd.unique(ann_df.tangram))\n",
    "scenes = sorted(pd.unique(ann_df.scene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baab5a9289954b60a61aa5b7ab9cb606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4070 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c703d7dbccdd45e88f64184e927a5828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4070 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7e74e7d9504a81b22033414e34aed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4070 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a3bddde4ee94a8aaf73931fe5ca3062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4070 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_array = ann_df[['item_identifyer', 'tangram', 'scene']].values\n",
    "\n",
    "raw_embeds_series = ann_df['raw_annotation'].progress_map(get_text_features)\n",
    "raw_embeds = np.stack(raw_embeds_series.to_list())\n",
    "\n",
    "clean_embeds_series = ann_df['clean_annotation'].progress_map(get_text_features)\n",
    "clean_embeds = np.stack(clean_embeds_series.to_list())\n",
    "\n",
    "head_embeds_series = ann_df['head_noun'].progress_map(get_text_features)\n",
    "head_embeds = np.stack(head_embeds_series.to_list())\n",
    "\n",
    "wn_embeds_series = ann_df['wn_lemma'].progress_map(get_text_features)\n",
    "wn_embeds = np.stack(wn_embeds_series.to_list())\n",
    "\n",
    "outfile = osp.join(output_dir, './ann_clip_embeddings.npz')\n",
    "np.savez(outfile, text_idx=idx_array, text_raw_emb=raw_embeds, text_clean_emb=clean_embeds, text_head_emb=head_embeds, text_wn_emb=wn_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting features for dense split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594b7b14719d4f8183c9de1004859ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1978 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting features for dense10 split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724fed6c64484e6998a44b34596e36d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def unpack_anns(list_of_annotations):\n",
    "    return [(i, x['whole']['wholeAnnotation']) for i, x in enumerate(list_of_annotations)]\n",
    "\n",
    "for split in 'dense', 'dense10':\n",
    "    print(f'extracting features for {split} split')\n",
    "\n",
    "    kilogram_path = osp.join(kilogram_dir, 'dataset', f'{split}.json')\n",
    "    kilogram_df = pd.read_json(kilogram_path).T\n",
    "\n",
    "    kilogram_df = kilogram_df.loc[tangrams]\n",
    "    kilogram_df['annotation_tuples'] = kilogram_df.annotations.map(unpack_anns)\n",
    "\n",
    "    exploded_kilogram_df = kilogram_df.explode('annotation_tuples')\n",
    "    exploded_kilogram_df['ann_idx'] = exploded_kilogram_df.annotation_tuples.map(lambda x: x[0])\n",
    "    exploded_kilogram_df['ann'] = exploded_kilogram_df.annotation_tuples.map(lambda x: x[1])\n",
    "\n",
    "    exploded_kilogram_df['item_identifyer'] = exploded_kilogram_df.apply(lambda x: f'{x.name}_{x.ann_idx}', axis=1)\n",
    "    exploded_kilogram_df = exploded_kilogram_df.reset_index().rename(columns={'index': 'tangram'})\n",
    "\n",
    "    # extract features\n",
    "\n",
    "    idx_array = exploded_kilogram_df[['item_identifyer', 'tangram', 'ann_idx']].values\n",
    "\n",
    "    embeds_series = exploded_kilogram_df['ann'].progress_map(get_text_features)\n",
    "    embeds = np.stack(embeds_series.to_list())\n",
    "\n",
    "    outfile = osp.join(output_dir, f'./{split}_clip_embeddings.npz')\n",
    "    np.savez(outfile, text_idx=idx_array, text_emb=embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18de60f4d9a741bbae3e6887d7d8cd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = sorted(glob('../generated_scenes/*'))\n",
    "\n",
    "idx_array = np.array([osp.split(path)[-1] for path in images])\n",
    "\n",
    "embeds_list = [get_features_for_img_path(path) for path in tqdm(images)]\n",
    "embeds = np.stack(embeds_list)\n",
    "\n",
    "outfile = osp.join(output_dir, './scene_clip_embeddings.npz')\n",
    "np.savez(outfile, img_idx=idx_array, img_emb=embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "context_representations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
