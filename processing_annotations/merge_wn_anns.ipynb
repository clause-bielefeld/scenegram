{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import sys\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "sys.path.append('../analyzing_annotations')\n",
    "from analysis_utils import read_ann_df, clean_wl, naming_div, display_img\n",
    "\n",
    "IMG_LOCATION=osp.abspath('../generated_items/')\n",
    "\n",
    "data_dir = osp.abspath('../collected_data/processed')\n",
    "kilogram_dir = osp.abspath('../kilogram')\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synset(s):\n",
    "    if type(s) == str:\n",
    "        return wn.synset(s)\n",
    "    return s\n",
    "\n",
    "def get_hypernyms(synset, include_self=True):\n",
    "    synset = make_synset(synset)\n",
    "    hypernyms = {synset} if include_self else set()\n",
    "    for hypernym in synset.hypernyms():\n",
    "        hypernyms |= set(get_hypernyms(hypernym))\n",
    "    return hypernyms | set(synset.hypernyms())\n",
    "\n",
    "def get_hyponyms(synset, include_self=True):\n",
    "    synset = make_synset(synset)\n",
    "    hyponyms = {synset} if include_self else set()\n",
    "    for hyponym in synset.hyponyms():\n",
    "        hyponyms |= set(get_hyponyms(hyponym))\n",
    "    return hyponyms | set(synset.hyponyms())\n",
    "\n",
    "def is_hypernym_of(synset, *reference_synsets, include_self=True):\n",
    "    synset = make_synset(synset)\n",
    "    reference_hypernyms = set()\n",
    "    for r in reference_synsets:\n",
    "        reference_hypernyms |= get_hypernyms(make_synset(r), include_self=include_self)\n",
    "    return synset in reference_hypernyms\n",
    "\n",
    "def is_hyponym_of(synset, *reference_synsets, include_self=True):\n",
    "    synset = make_synset(synset)\n",
    "    reference_hyponyms = set()\n",
    "    for r in reference_synsets:\n",
    "        reference_hyponyms |= get_hyponyms(make_synset(r), include_self=include_self)\n",
    "    return synset in reference_hyponyms\n",
    "\n",
    "def get_first_lemma(synset):\n",
    "    return make_synset(synset).lemma_names()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = osp.join(data_dir, 'valid_processed_collected_data.csv')\n",
    "ann_df = read_ann_df(input_file)\n",
    "ann_df.head_noun = ann_df.head_noun.apply(lambda x: x.split('/')[0].strip())\n",
    "\n",
    "tangrams, scenes = zip(*ann_df.index)\n",
    "tangrams = sorted(set(tangrams))\n",
    "scenes = sorted(set(scenes))\n",
    "\n",
    "tangram2idx = {t:i for i, t in enumerate(tangrams)}\n",
    "idx2tangram = {i:t for t, i in tangram2idx.items()}\n",
    "\n",
    "ann_df = ann_df.rename(columns={'comments': 'ann_comments'})\n",
    "\n",
    "display(ann_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = glob(osp.join(data_dir, 'valid_processed_synsets_*_human.csv'))\n",
    "wn_dfs = [pd.read_csv(input_file, index_col=0) for input_file in input_files]\n",
    "wn_anns = pd.concat(wn_dfs).rename(columns={'comments': 'wn_comments'})\n",
    "\n",
    "assert set(wn_anns.item_identifyer.unique()) == set(ann_df.item_identifyer.unique())\n",
    "\n",
    "# replace empty synsets with entity.n.01\n",
    "wn_anns.selected_synset = wn_anns.selected_synset.fillna('entity.n.01')\n",
    "# map synset strings to synsets\n",
    "wn_anns['selected_synset_obj'] = wn_anns.selected_synset.map(wn.synset)\n",
    "# update definitions\n",
    "wn_anns.synset_definition = wn_anns.selected_synset_obj.map(lambda x: x.definition())\n",
    "# normalize head noun using WordNet\n",
    "wn_anns['wn_lemma'] = wn_anns.selected_synset_obj.map(get_first_lemma)\n",
    "# replace head_noun entries with corrected versions\n",
    "corrected_mask = ~wn_anns.corrected_head_noun.isna()\n",
    "wn_anns[corrected_mask].head_noun = wn_anns[corrected_mask].corrected_head_noun\n",
    "\n",
    "wn_anns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_columns = [\n",
    "    'item_identifyer', 'tangram', 'scene', 'raw_annotation', 'ann_comments', \n",
    "    'tangram_id', 'kilogram_snd', 'item_id', 'workspace_name', 'partition_name', \n",
    "    'dataset_name', 'tangram_pos', 'image_url', 'meta_record', 'user_name', \n",
    "    'status', 'time', 'valid', 'order_idx']\n",
    "\n",
    "wn_columns = [\n",
    "    'item_identifyer', 'clean_annotation', 'head_noun', 'wn_lemma',\n",
    "    'selected_synset', 'synset_definition', 'wn_comments']\n",
    "\n",
    "col_order = [\n",
    "    'item_identifyer', 'tangram', 'scene', 'raw_annotation',  'clean_annotation',  \n",
    "    'head_noun', 'wn_lemma',  'selected_synset',  'synset_definition',  'tangram_id', \n",
    "    'item_id',  'image_url',  'ann_comments',  'wn_comments', 'kilogram_snd',  \n",
    "    'workspace_name',  'partition_name', 'dataset_name',  'tangram_pos',  'user_name', \n",
    "    'meta_record',  'time',  'order_idx',  'status',  'valid']\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    left=ann_df.reset_index()[ref_columns],\n",
    "    right=wn_anns[wn_columns],\n",
    "    left_on='item_identifyer',\n",
    "    right_on='item_identifyer'\n",
    ")[col_order]\n",
    "\n",
    "out_path = osp.join(data_dir, 'final_processed_data.csv')\n",
    "merged_df.to_csv(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df.selected_synset == 'entity.n.01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_anns.wn_lemma.value_counts().iloc[:30][::-1].plot(kind='barh')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\# Entries per Semantic Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_per_synset = {\n",
    "    'person.n.01': False,\n",
    "    'animal.n.01': False,\n",
    "    'artifact.n.01': False\n",
    "}\n",
    "\n",
    "for ref_synset in entries_per_synset.keys():\n",
    "    ref_hyponyms = get_hyponyms(ref_synset)\n",
    "    hyponym_mask = wn_anns.selected_synset_obj.isin(ref_hyponyms)\n",
    "    hyponym_entries = wn_anns[hyponym_mask]\n",
    "    entries_per_synset[ref_synset] = len(hyponym_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_per_synset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\# Hyponyms per Synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset_query = {s for s in wn.all_synsets() if s.pos() == 'n'}\n",
    "wn_analysis_results = list()\n",
    "pbar = tqdm()\n",
    "\n",
    "while len(synset_query) > 0:\n",
    "    \n",
    "    ref_synset = next(iter(synset_query))\n",
    "    \n",
    "    ref_hyponyms = get_hyponyms(ref_synset)\n",
    "\n",
    "    is_hyponym_mask = wn_anns.selected_synset_obj.map(lambda x: x in ref_hyponyms)\n",
    "    n_hyponyms = len(wn_anns[is_hyponym_mask])\n",
    "    hyponym_ratio = n_hyponyms / len(wn_anns)\n",
    "    \n",
    "    if n_hyponyms > 0:\n",
    "        \n",
    "        wn_analysis_results.append({\n",
    "            'synset': ref_synset.name(),\n",
    "            'n_hyponyms': n_hyponyms,\n",
    "            'hyponym_ratio': hyponym_ratio\n",
    "        })\n",
    "        \n",
    "        synset_query.remove(ref_synset)\n",
    "        \n",
    "    else:\n",
    "        synset_query -= ref_hyponyms\n",
    "        \n",
    "    pbar.update()\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(wn_analysis_results)\n",
    "results_df = results_df.set_index('synset').sort_values(by='hyponym_ratio', ascending=False)\n",
    "results_df.iloc[:25]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "context_representations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
